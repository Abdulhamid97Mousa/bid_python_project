# BDI-RL Real-Time Agent Communication System

## üéØ Project Purpose & Overview

This project creates a **real-time bridge between Machine Learning/Reinforcement Learning models and Belief-Desire-Intention (BDI) agents**. It enables Python-based ML/RL algorithms to observe agent behavior as it happens and dynamically modify agent plans based on learned policies, without requiring agent restarts or file system operations.

### üî¨ Research Context & Applications

**Academic Use Cases:**
- **Adaptive Multi-Agent Systems**: RL models learn optimal coordination strategies and update agent behaviors in real-time
- **Dynamic Resource Allocation**: ML models observe resource usage patterns and adjust agent allocation strategies
- **Emergent Behavior Studies**: Researchers can inject learned behaviors into agent populations to study emergence
- **Human-AI Collaboration**: ML models adapt agent strategies based on human interaction patterns

**Industry Applications:**
- **Smart Manufacturing**: RL models optimize production workflows by updating robot agent plans in real-time
- **Autonomous Vehicle Coordination**: ML models adjust traffic agent behaviors based on learned traffic patterns
- **Financial Trading Systems**: RL algorithms update trading agent strategies based on market observations
- **Game AI**: ML models create adaptive NPCs that modify behavior based on player actions

### üß† Technical Innovation

**The Core Problem**: Traditional BDI systems have static plans defined at startup. When ML/RL models learn new optimal strategies, there's no way to update running agents without:
- Stopping and restarting agents (losing state)
- Modifying files and reloading (slow, error-prone)
- Manual intervention (not scalable)

**Our Solution**: A real-time messaging system that allows ML/RL models to:
1. **Observe** agent states and actions through monitoring
2. **Learn** optimal strategies from observed behaviors  
3. **Decide** when and how to modify agent plans
4. **Inject** new plans directly into running agents via Protocol Buffer messages
5. **Scale** to hundreds of agents across distributed systems

### üèó System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    Protocol Buffers    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     ML/RL Model         ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄover ZeroMQ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ      BDI Agents         ‚îÇ
‚îÇ   (Python/TensorFlow)   ‚îÇ                         ‚îÇ   (AgentSpeak/SPADE)    ‚îÇ
‚îÇ                         ‚îÇ                         ‚îÇ                         ‚îÇ
‚îÇ ‚Ä¢ Observes environments ‚îÇ                         ‚îÇ ‚Ä¢ Receives mutations    ‚îÇ
‚îÇ ‚Ä¢ Learns from feedback  ‚îÇ                         ‚îÇ ‚Ä¢ Updates plans runtime ‚îÇ
‚îÇ ‚Ä¢ Computes optimal acts ‚îÇ                         ‚îÇ ‚Ä¢ Continues execution   ‚îÇ
‚îÇ ‚Ä¢ Publishes plan updates‚îÇ                         ‚îÇ ‚Ä¢ No restarts needed    ‚îÇ
‚îÇ                         ‚îÇ                         ‚îÇ                         ‚îÇ
‚îÇ Deep Q-Network (DQL)    ‚îÇ   Sub-10ms latency     ‚îÇ Jason/AgentSpeak        ‚îÇ
‚îÇ Policy Gradient         ‚îÇ   Type-safe messages    ‚îÇ SPADE-BDI Framework     ‚îÇ
‚îÇ Actor-Critic            ‚îÇ   Schema validation     ‚îÇ Multi-agent coordination‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üéÆ Simulation Overview

The system simulates a **dynamic environment where RL agents learn and BDI agents execute**:

1. **Environment Phase**: BDI agents operate in their domain (trading, manufacturing, gaming, etc.)
2. **Observation Phase**: ML/RL models monitor agent performance and environmental changes
3. **Learning Phase**: RL algorithms update their understanding of optimal strategies  
4. **Adaptation Phase**: ML models send new plan mutations to BDI agents in real-time
5. **Execution Phase**: Agents immediately incorporate new plans without interruption
6. **Feedback Loop**: The cycle continues with improved performance

**Example Simulation Flow:**
```
Time T0: Agent executes default trading strategy
Time T1: RL model observes 60% success rate  
Time T2: RL model learns pattern in market data
Time T3: RL model computes improved strategy
Time T4: RL model sends new plan via ZeroMQ
Time T5: Agent immediately adopts new strategy  
Time T6: Agent achieves 85% success rate
Time T7: Cycle repeats with new observations
```

## üõ† Complete Setup Guide

### üöÄ AUTOMATED SETUP (RECOMMENDED)

**New!** Use the automated setup script that handles all installation steps:

```bash
# Run the complete automated setup
python setup_system.py
```

This script will:
- ‚úÖ Install protoc with multiple fallback methods (including offline support)
- ‚úÖ Create and setup Python virtual environment  
- ‚úÖ Install all dependencies from requirements.txt
- ‚úÖ Generate Protocol Buffer stubs automatically
- ‚úÖ Organize legacy files
- ‚úÖ Setup Cargo PATH if needed
- ‚úÖ Run system tests
- ‚úÖ Provide clear next steps

**If automated setup fails, continue with manual setup below:**

---

### Step 1: Install Protocol Buffer Compiler (protoc)

#### For Windows/Git Bash:

1. **Download protoc:**
   ```bash
   # Create a tools directory
   mkdir -p ~/tools/protoc
   cd ~/tools/protoc
   
   # Download protoc for Windows (if GitHub is accessible)
   curl -LO https://github.com/protocolbuffers/protobuf/releases/download/v25.1/protoc-25.1-win64.zip
   
   # If GitHub is blocked, download manually from:
   # https://github.com/protocolbuffers/protobuf/releases/download/v25.1/protoc-25.1-win64.zip
   ```

2. **Extract and setup:**
   ```bash
   # Extract the zip file
   unzip protoc-25.1-win64.zip
   
   # Add to PATH (add this to your ~/.bashrc or ~/.bash_profile)
   echo 'export PATH="$HOME/tools/protoc/bin:$PATH"' >> ~/.bashrc
   source ~/.bashrc
   ```

3. **Alternative - Use Chocolatey (if available):**
   ```bash
   choco install protoc
   ```

4. **Alternative - Manual Installation:**
   - Download `protoc-25.1-win64.zip` manually
   - Extract to `C:\tools\protoc\`
   - Add `C:\tools\protoc\bin` to Windows PATH environment variable
   - Restart Git Bash

#### For Linux/WSL:
```bash
sudo apt-get update
sudo apt-get install -y protobuf-compiler
```

#### For macOS:
```bash
brew install protobuf
```

### Step 2: Verify protoc Installation
```bash
protoc --version
# Should output: libprotoc 25.1.0 (or similar)
```

### Step 3: Setup Python Environment

1. **Create virtual environment:**
   ```bash
   python -m venv .venv
   
   # Activate (Git Bash/Linux/macOS)
   source .venv/bin/activate
   
   # Activate (Windows PowerShell)
   .venv\Scripts\Activate.ps1
   
   # Activate (Windows CMD)
   .venv\Scripts\activate.bat
   ```

2. **Install dependencies:**
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

### Step 4: Generate Protocol Buffer Code
```bash
# Compile the .proto file to generate Python classes
protoc --python_out=. mutations.proto

# Verify the generated file exists
ls -la mutations_pb2.py
```

### Step 5: Test the System
```bash
# Run the test suite to verify everything works
python test_system.py
```

## üéÆ How to Run Simulations

### üöÄ **Quick Start Simulation**

```bash
# 1. Verify system is ready
python test_system.py

# 2. Run live 30-second demo
python demo_system.py

# 3. Test communication fixes
python test_port_fix.py
```

### üî¨ **Step-by-Step Simulation Execution**

#### **Simulation 1: Basic RL-Agent Communication**
```bash
# Terminal 1: Start the ML/RL model
python rl_plan_sender.py
# Output: RL Plan Sender started on port 5555
#         Starting continuous RL plan generation...
#         Published plan for context: get_action_context, action: action_rl1

# Terminal 2: Start BDI agent (wait 1 second after Terminal 1)
python simple_agent_demo.py  
# Output: Agent demo_agent_01 connected to ZeroMQ on port 5555
#         Agent demo_agent_01 - Received new plan:
#         Agent demo_agent_01 - Plan added successfully! Total runtime plans: 1
```

#### **Simulation 2: Multiple Agent Coordination**
```bash
# Terminal 1: Start RL model
python rl_plan_sender.py

# Terminal 2: Start Agent 1
python -c "
from simple_agent_demo import SimpleBDIAgent
agent = SimpleBDIAgent('agent_01')
agent.load_initial_state()
agent.setup_zmq_connection()
agent.run()
"

# Terminal 3: Start Agent 2 
python -c "
from simple_agent_demo import SimpleBDIAgent  
agent = SimpleBDIAgent('agent_02')
agent.load_initial_state() 
agent.setup_zmq_connection()
agent.run()
"
```

#### **Simulation 3: Custom ML/RL Integration**
```bash
# 1. Modify rl_plan_sender.py with your model
# 2. Update Q_table with your state-action pairs
# 3. Run simulation
python rl_plan_sender.py

# Watch agents adapt to your ML model's decisions in real-time
```

### üìä **Understanding Simulation Output**

#### **Expected RL Model Output:**
```
RL Plan Sender started on port 5555
Starting continuous RL plan generation...
Published plan for context: get_action_context, action: action_rl1
Published plan for context: other_context, action: action_rl2
```

#### **Expected Agent Output:**
```
Simple BDI Agent Demo - ZeroMQ Protocol Buffer Testing
Loading initial state for agent demo_agent_01...
  Initial goal: !start.
  Initial belief: state(default_state).
  Initial belief: energy(100).
Agent demo_agent_01 connected to ZeroMQ on port 5555
Agent demo_agent_01 - Received new plan:
  Priority: 1
  Plan: +!get_action : context(C) & C == "get_action_context" <-
    .print("RL chosen: action_rl1");
    -context(C).
Agent demo_agent_01 - Plan added successfully! Total runtime plans: 1
```

### ‚ö° **Automated Simulation Options**

#### **Option 1: Complete Workflow**
```bash
python start_workflow.py
# Starts both RL sender and BDI agents automatically
```

#### **Option 2: Manual Component Control**
```bash
# Terminal 1: ML/RL Model
python rl_plan_sender.py

# Terminal 2: BDI Agents
python simple_agent_demo.py
```

#### **Option 3: SPADE-BDI Integration (Advanced)**
```bash
# Terminal 1: RL Model
python rl_plan_sender.py

# Terminal 2: Full BDI Agents (requires XMPP server)
python agent_launcher.py
```

### üéØ **Simulation Success Indicators**

**‚úÖ Successful Communication:**
- Agent shows "Plan added successfully!" messages
- No "Failed to parse mutation" errors
- Real-time message delivery (<10ms latency)
- Multiple agents receiving same mutations

**‚ùå Common Issues:**
- Port mismatch: "Address already in use"
- Parse errors: "Failed to parse mutation"
- Connection failures: Agent timeout messages

### üîÑ **Simulation Loop Explanation**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Simulation Cycle                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Environment Setup                                        ‚îÇ
‚îÇ    ‚Ä¢ BDI agents initialize with agent.asl                  ‚îÇ 
‚îÇ    ‚Ä¢ RL model loads Q-table or neural network              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 2. Observation Phase                                        ‚îÇ
‚îÇ    ‚Ä¢ RL model observes agent performance                   ‚îÇ
‚îÇ    ‚Ä¢ Environmental state changes are detected              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 3. Learning Phase                                           ‚îÇ
‚îÇ    ‚Ä¢ RL algorithm updates policy based on rewards          ‚îÇ
‚îÇ    ‚Ä¢ Q-values or neural weights are adjusted               ‚îÇ
‚îÇ                                                             ‚îÇ  
‚îÇ 4. Decision Phase                                           ‚îÇ
‚îÇ    ‚Ä¢ RL model computes optimal action for current state    ‚îÇ
‚îÇ    ‚Ä¢ New strategy is formulated as AgentSpeak plan         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 5. Communication Phase                                      ‚îÇ 
‚îÇ    ‚Ä¢ PlanMutation message created with new strategy        ‚îÇ
‚îÇ    ‚Ä¢ Message sent via ZeroMQ to all subscribed agents      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 6. Execution Phase                                          ‚îÇ
‚îÇ    ‚Ä¢ Agents receive and validate new plan                  ‚îÇ
‚îÇ    ‚Ä¢ Plan is integrated into agent's reasoning cycle       ‚îÇ
‚îÇ    ‚Ä¢ Agent begins executing new strategy immediately       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 7. Feedback Phase                                           ‚îÇ
‚îÇ    ‚Ä¢ Agent performance is monitored                        ‚îÇ
‚îÇ    ‚Ä¢ Results feed back to RL model for next iteration      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure & File Purposes

### üÜï **Core System Files (New Architecture)**

#### **Communication Layer**
- **`mutations.proto`** - Protocol Buffer schema definition
  - Defines `PlanMutation` and `BeliefMutation` message structures
  - Ensures type safety and forward compatibility
  - Language-agnostic schema (can generate code for Go, Rust, Java, etc.)

- **`mutations_pb2.py`** - Generated Python Protocol Buffer classes
  - Auto-generated from `mutations.proto` 
  - Provides `SerializeToString()` and `ParseFromString()` methods
  - Handles message validation and encoding/decoding

#### **ML/RL Publisher Side**
- **`rl_plan_sender.py`** - Main ML/RL model interface
  - **Purpose**: Publishes plan mutations to BDI agents via ZeroMQ
  - **Key Functions**: 
    - Loads Q-table or neural network models
    - Converts ML decisions to AgentSpeak plans
    - Broadcasts mutations to all subscribed agents
  - **Integration Point**: Replace Q_table with your ML/RL model here
  - **Output**: Real-time plan updates every 2 seconds (configurable)

#### **Agent Subscriber Side**
- **`simple_agent_demo.py`** - Standalone BDI agent demonstration
  - **Purpose**: Shows ZeroMQ communication without requiring SPADE-BDI
  - **Key Functions**:
    - Connects to ZeroMQ publisher on port 5555
    - Parses incoming PlanMutation/BeliefMutation messages
    - Validates AgentSpeak plan syntax
    - Demonstrates real-time plan integration
  - **Use Case**: Testing, debugging, and demonstrations

- **`mutate_behaviour.py`** - SPADE-BDI integration behavior
  - **Purpose**: Enables production BDI agents to receive real-time mutations
  - **Key Functions**:
    - Implements SPADE CyclicBehaviour for continuous listening
    - Integrates with SPADE-BDI agent reasoning cycle
    - Handles asynchronous message processing
  - **Use Case**: Production multi-agent systems with XMPP communication

- **`agent_launcher.py`** - SPADE-BDI agent orchestrator
  - **Purpose**: Launches and manages multiple SPADE-BDI agents
  - **Key Functions**:
    - Creates MutableBDIAgent instances with mutation capabilities
    - Loads initial state from agent.asl
    - Manages agent lifecycle and monitoring
  - **Requirements**: XMPP server for SPADE-BDI communication

#### **System Orchestration**
- **`start_workflow.py`** - Master system coordinator
  - **Purpose**: Orchestrates the complete BDI-RL workflow
  - **Key Functions**:
    - Starts RL sender and agents in correct sequence
    - Handles process management and error recovery
    - Provides unified control interface
  - **Use Case**: Production deployments and integrated testing

#### **Testing & Validation**
- **`test_system.py`** - Comprehensive system validator
  - **Purpose**: Validates all system components are working correctly
  - **Tests Performed**:
    - File structure verification
    - Protocol Buffer serialization/deserialization
    - ZeroMQ socket creation and communication
    - Import dependency checks
  - **Output**: Pass/fail status with detailed error reporting

- **`test_port_fix.py`** - Communication debugging tool
  - **Purpose**: Tests specific fixes for port mismatch and slow-joiner issues
  - **Key Functions**:
    - Validates port synchronization between publisher/subscriber
    - Tests slow-joiner mitigation (0.5s delay)
    - Monitors message flow and parse success rates
  - **Use Case**: Debugging ZeroMQ communication issues

- **`demo_system.py`** - Live system demonstration
  - **Purpose**: Provides 30-second live demo of RL-Agent communication
  - **Key Functions**:
    - Automated startup of RL sender and agent
    - Real-time output monitoring
    - Graceful shutdown and status reporting
  - **Use Case**: Presentations, demos, and system validation

#### **Setup & Installation**
- **`setup_system.py`** - Automated installation script
  - **Purpose**: Handles complete system setup with fallback options
  - **Key Functions**:
    - Installs protoc with multiple download sources
    - Creates and configures Python virtual environment
    - Generates Protocol Buffer stubs
    - Configures PATH variables (protoc, cargo)
    - Runs system validation tests
  - **Fallbacks**: Chocolatey, manual download, environment variable setup

- **`generate_protobuf_stubs.py`** - Protocol Buffer stub generator
  - **Purpose**: Creates mutations_pb2.py when protoc is unavailable
  - **Key Functions**:
    - Generates protobuf-compatible classes using Python library
    - Provides SerializeToString/ParseFromString methods
    - Includes JSON fallback for simplified serialization
  - **Use Case**: Environments where protoc installation fails

### üìö **Documentation Files**

- **`README.md`** - Complete project documentation (this file)
  - Project overview, installation, usage, and troubleshooting
  - Step-by-step simulation instructions
  - File-by-file purpose explanations

- **`README_new_system.md`** - Technical implementation details
  - Deep-dive into Protocol Buffer + ZeroMQ architecture
  - Performance analysis and benchmarking data
  - Advanced configuration and scaling guidance

- **`requirements.txt`** - Python dependency specification
  - `spade-bdi==0.3.2` - BDI framework for multi-agent systems
  - `python-agentspeak>=1.0` - AgentSpeak interpreter
  - `protobuf>=4.25.0` - Protocol Buffer runtime
  - `pyzmq>=25.1.1` - ZeroMQ Python bindings

### üèõÔ∏è **Core Assets (Domain Logic)**

- **`agent.asl`** - Initial agent knowledge base
  - **Purpose**: Defines starting beliefs, goals, and plans for BDI agents  
  - **Content**: AgentSpeak syntax with initial state, energy levels, goals
  - **Role**: Bootstraps agents before they receive ML-generated mutations
  - **Format**: Standard Jason/AgentSpeak syntax

- **`dql_module.py`** - Deep Q-Learning interface
  - **Purpose**: Provides RL environment interface for Q-learning algorithms
  - **Integration**: Can be connected to `rl_plan_sender.py` for neural network-based RL
  - **Use Case**: Research experiments with deep reinforcement learning

- **`env.py`** - Environment definitions and state management
  - **Purpose**: Defines the simulation environment and state space
  - **Content**: Environment dynamics, reward functions, state transitions
  - **Integration**: Works with both DQL module and BDI agents

- **`main.py`** - Legacy entry point
  - **Purpose**: Original system entry point (now deprecated)
  - **Status**: Kept for backward compatibility
  - **Replacement**: Use `start_workflow.py` for new system

### üì¶ **Legacy Files (Moved to legacy_files/)**

These files represent the old file-based architecture and are preserved for reference:

- **`need_plan.txt`** - Old request signaling mechanism
- **`new_plan.asl`** - Old plan storage file  
- **`merge_plan.py`** - Old plan file merger utility
- **`rl_plan_generator.py`** - Old file-polling RL module
- **`run_bdi_rl_workflow.py`** - Old workflow orchestrator
- **`run_bdi_rl_wrapper.py`** - Old system wrapper

### üéØ **File Usage Matrix**

| Simulation Type | Primary Files | Optional Files |
|----------------|---------------|----------------|
| **Quick Demo** | `demo_system.py` | `test_system.py` |
| **Basic RL-Agent** | `rl_plan_sender.py`, `simple_agent_demo.py` | `agent.asl` |
| **Multi-Agent** | `rl_plan_sender.py`, `agent_launcher.py`, `mutate_behaviour.py` | `agent.asl` |
| **Custom ML Integration** | `rl_plan_sender.py` (modified), `simple_agent_demo.py` | `dql_module.py`, `env.py` |
| **Production Deployment** | `start_workflow.py`, `mutate_behaviour.py`, `agent_launcher.py` | All core files |
| **System Development** | `test_system.py`, `generate_protobuf_stubs.py` | `setup_system.py` |

### üîÑ **File Dependencies**

```
setup_system.py
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îú‚îÄ‚îÄ mutations.proto
    ‚îî‚îÄ‚îÄ generate_protobuf_stubs.py
        ‚îî‚îÄ‚îÄ mutations_pb2.py

rl_plan_sender.py
    ‚îú‚îÄ‚îÄ mutations_pb2.py
    ‚îú‚îÄ‚îÄ zmq (pyzmq)
    ‚îî‚îÄ‚îÄ [Your ML/RL Model]

simple_agent_demo.py
    ‚îú‚îÄ‚îÄ mutations_pb2.py
    ‚îú‚îÄ‚îÄ zmq (pyzmq)
    ‚îî‚îÄ‚îÄ agent.asl (optional)

start_workflow.py
    ‚îú‚îÄ‚îÄ rl_plan_sender.py
    ‚îî‚îÄ‚îÄ agent_launcher.py OR simple_agent_demo.py

test_system.py
    ‚îú‚îÄ‚îÄ mutations_pb2.py
    ‚îú‚îÄ‚îÄ All core system files
    ‚îî‚îÄ‚îÄ requirements validation
```

## üîß Configuration & Customization

### Modifying the ML/RL Model Behavior

Edit `rl_plan_sender.py` to integrate your ML/RL model:

```python
# Replace the simple Q-table with your ML model
def get_action_from_model(context, observations):
    # Your ML/RL model logic here
    # model_output = your_model.predict(context, observations)
    return "optimal_action"

# Publish plans based on ML decisions
def publish_ml_decision(context, action):
    plan_text = f'''
    +!execute_action : context({context}) <-
        .print("ML model chose: {action}");
        perform_action({action});
        update_context.
    '''
    
    mutation = PlanMutation(
        op=PlanMutation.ADD,
        prio=1,
        plan=plan_text
    )
    socket.send(mutation.SerializeToString())
```

### Adding New Message Types

1. **Edit `mutations.proto`:**
   ```protobuf
   message CustomMutation {
     enum Op { ADD = 0; REMOVE = 1; UPDATE = 2; }
     Op op = 1;
     string custom_field = 2;
     // Add new fields with unique tag numbers
   }
   ```

2. **Regenerate Python code:**
   ```bash
   protoc --python_out=. mutations.proto
   ```

3. **Use in your code:**
   ```python
   from mutations_pb2 import CustomMutation
   
   msg = CustomMutation(op=CustomMutation.ADD, custom_field="value")
   socket.send(msg.SerializeToString())
   ```

## üö® Troubleshooting

### Common Issues and Solutions

#### 1. `protoc: command not found`
```bash
# Check if protoc is installed
which protoc

# If not found, make sure PATH includes protoc/bin
echo $PATH | grep protoc

# Add to PATH if missing
export PATH="$HOME/tools/protoc/bin:$PATH"
echo 'export PATH="$HOME/tools/protoc/bin:$PATH"' >> ~/.bashrc
```

#### 2. `.cargo` PATH Export Issues
If you see Rust/Cargo related errors:
```bash
# Add Cargo to PATH
echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```

#### 3. `ImportError: No module named mutations_pb2`
```bash
# Make sure protoc generated the file
ls -la mutations_pb2.py

# If missing, regenerate
protoc --python_out=. mutations.proto

# Check Python can import it
python -c "from mutations_pb2 import PlanMutation; print('Import successful')"
```

#### 4. ZeroMQ Port Issues
```bash
# Check if port 5555 is in use
netstat -an | grep 5555

# Change port in rl_plan_sender.py if needed
# socket.bind("tcp://*:5556")  # Use different port
```

#### 5. SPADE-BDI Import Errors
```bash
# Install SPADE-BDI specifically
pip install spade-bdi==0.3.2

# If still failing, try development version
pip install git+https://github.com/javipalanca/spade-bdi.git
```

#### 6. Windows Permission Issues
```bash
# Run Git Bash as Administrator if needed
# Or use Windows Subsystem for Linux (WSL)
```

### Debug Mode
Enable verbose logging to diagnose issues:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üìä Performance Expectations

| Metric | File-based (Old) | Protocol Buffers + ZeroMQ (New) |
|--------|------------------|----------------------------------|
| Latency | 2-5 seconds | < 10 milliseconds |
| Throughput | 1 plan/second | 1000+ plans/second |
| Agents | 1 (single process) | 100+ (distributed) |
| Reliability | File corruption risk | Type-safe, atomic |
| Scalability | Single machine | Network distributed |

## üîÑ Migration from Old System

If you have existing code using the old file-based system:

1. **Backup old files** (already moved to `legacy_files/`)
2. **Update RL integration:**
   - Replace file writes with ZeroMQ publishing
   - Use `PlanMutation` instead of writing `.asl` files
3. **Update agent startup:**
   - Use `agent_launcher.py` instead of manual AgentSpeak execution
   - Agents now receive mutations in real-time

## üéØ Use Cases

### 1. Adaptive Game AI
```python
# ML model observes game state, sends new strategies to agents
if enemy_behavior_changed:
    new_plan = '+!adapt_strategy : enemy_type(aggressive) <- defensive_mode.'
    publish_plan_mutation(new_plan)
```

### 2. Dynamic Resource Allocation
```python
# RL model learns optimal resource distribution
optimal_allocation = rl_model.get_allocation(current_state)
belief_update = f'resource_priority({optimal_allocation})'
publish_belief_mutation(belief_update)
```

### 3. Real-time Trading Systems
```python
# ML model detects market patterns, updates trading agents
if market_trend == 'bullish':
    trading_plan = '+!trade : market(bullish) <- buy_signal; execute_trade.'
    publish_plan_mutation(trading_plan)
```

## üî¨ Next Steps

1. **Integrate your ML/RL model** with `rl_plan_sender.py`
2. **Scale testing** with multiple agents
3. **Add persistence** for plan mutations
4. **Implement security** for production deployment
5. **Monitor performance** with metrics collection

---

## üé¨ Live Demo

**See the system in action:**

```bash
# Complete working demo (30 seconds)
python demo_system.py
```

This shows real-time ML/RL ‚Üí Agent communication with Protocol Buffers + ZeroMQ!

**üöÄ Quick Start Summary:**
1. **Automated Setup**: `python setup_system.py`
2. **Test System**: `python test_system.py`
3. **Live Demo**: `python demo_system.py`
4. **Manual Components**: `python rl_plan_sender.py` + `python simple_agent_demo.py`

**Need help?** Check the troubleshooting section above or run `python test_system.py` to diagnose issues.
